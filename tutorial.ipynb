{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![foundation](https://cdn.midjourney.com/12b38a83-119d-4c5c-949d-2fa3ebfe0140/0_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Applications\n",
    "3. Training a Model\n",
    "    - NumPy Example\n",
    "    - PyTorch Example\n",
    "5. Transfer Learning\n",
    "    - NumPy Example\n",
    "    - PyTorch Example\n",
    "6. Transformers\n",
    "    - Tokenization\n",
    "    - Training Process\n",
    "    - Prompting\n",
    "    - Fine-Tuning\n",
    "    - RLHF\n",
    "    - Challenges\n",
    "7. Serving\n",
    "8. What we are up to at Seldon\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundation models are large-scale machine learning models trained on vast amounts of data, \n",
    "enabling them to adapt to various downstream tasks with minimal fine-tuning. These models \n",
    "have turned the field of AI upside down by providing a powerful starting point for a wide \n",
    "range of applications powered with automation intelligent components in them. Some \n",
    "key characteristics of foundation models include:\n",
    "\n",
    "1. Pre-training on massive datasets\n",
    "2. Ability to transfer knowledge to new tasks\n",
    "3. Capability to handle multiple modalities (e.g., text, images, audio, tabular)\n",
    "4. Scalability and efficiency in training and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundation models have found applications across various domains, including:\n",
    "\n",
    "1. Natural Language Processing (NLP): Language translation, text summarization, sentiment analysis, question answering, and more.\n",
    "2. Computer Vision: Image classification, object detection, semantic segmentation, and image generation.\n",
    "3. Speech Recognition: Automatic speech recognition, speaker identification, and voice synthesis.\n",
    "4. Multimodal Learning: Combining multiple modalities, such as text and images, for tasks like image captioning and visual question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NumPy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we define a `TinyImageClassifier` class that encapsulates the model parameters, \n",
    "forward pass, backward pass, training loop, and prediction method.\n",
    "\n",
    "1. The `__init__` method initializes the model parameters (W1, b1, W2, b2) based on the input size, hidden size, and number of classes. Think of the neural network as a team of workers in a factory. Each worker (neuron) has a specific task and is connected to other workers. The weights represent the strength of the connections between workers, and the biases represent the individual preferences of each worker. Initially, the workers are assigned random tasks (weights) and have no specific preferences (biases). As the training progresses, the workers learn and adapt their tasks and preferences based on the feedback they receive.\n",
    "2. The forward method performs the forward pass, computing the output scores given the input data X. Imagine the neural network as a series of conveyor belts in a factory. The input data (X) is placed on the first conveyor belt, and as it moves along, it undergoes transformations. The weights (W1 and W2) represent the machinery that processes the data, and the biases (b1 and b2) are additional adjustments made to the data. The ReLU activation function is like a quality control checkpoint that filters out any negative values. Finally, the processed data reaches the end of the conveyor belt, resulting in the output predictions.\n",
    "3. The backward method computes the gradients of the loss with respect to the weights and biases using \n",
    "[the chain rule](https://machinelearningmastery.com/the-chain-rule-of-calculus-for-univariate-and-multivariate-functions/). Think of the loss as a measure of how far off the mark the model's predictions are from the true targets. Imagine you are an archer aiming at a target. The output scores are like the arrows you shoot, and the true labels are the bullseye. The softmax function normalizes the scores, similar to adjusting the tension in your bow to ensure the arrows land on the target. The cross-entropy loss measures the distance between your arrows and the bullseye. The smaller the loss, the closer your arrows are to the center of the target.\n",
    "4. The train method implements the training loop, where it iteratively performs the forward pass, computes the loss, \n",
    "performs the backward pass, and updates the weights and biases using gradient descent. Think of the training loop as a fitness program. Each epoch is like a training session where you exercise (perform forward and backward passes) to improve your fitness level (reduce the loss). The weights and biases are like your muscles, and the gradient descent is the training regimen that strengthens them. After each session, you assess your progress (loss) to see how much you've improved. You repeat this process for a set number of sessions (epochs) until you reach your desired fitness level (minimum loss).\n",
    "5. The predict method allows you to make predictions on new samples. It takes the input data X, performs the forward \n",
    "pass, applies the softmax function to the output scores, and returns the predicted class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TinyImageClassifier:\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, num_classes) * 0.01\n",
    "        self.b2 = np.zeros((1, num_classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.h = np.maximum(0, np.dot(X, self.W1) + self.b1)\n",
    "        scores = np.dot(self.h, self.W2) + self.b2\n",
    "        return scores\n",
    "    \n",
    "    def backward(self, X, y, scores):\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        dscores = probs\n",
    "        dscores[range(num_samples), y] -= 1\n",
    "        dscores /= num_samples\n",
    "        \n",
    "        dW2 = np.dot(self.h.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        \n",
    "        dh = np.dot(dscores, self.W2.T)\n",
    "        dh[self.h <= 0] = 0\n",
    "        \n",
    "        dW1 = np.dot(X.T, dh)\n",
    "        db1 = np.sum(dh, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            scores = self.forward(X)\n",
    "            \n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            correct_logprobs = -np.log(probs[range(num_samples), y])\n",
    "            loss = np.sum(correct_logprobs) / num_samples\n",
    "            \n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, scores)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = self.forward(X)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this model, you can create an instance of the TinyImageClassifier class, specifying the input size, hidden size, \n",
    "and number of classes. Then, you can call the train method to train the model on your training data X and labels y.\n",
    "\n",
    "After training, we can use the predict method to make predictions on new samples X_test. The predicted class labels will be returned as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "X = np.random.randn(num_samples, input_size)\n",
    "y = np.random.randint(0, num_classes, size=(num_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = TinyImageClassifier(input_size, hidden_size, num_classes)\n",
    "model.train(X, y, num_epochs, learning_rate)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = np.random.randn(10, input_size)\n",
    "\n",
    "# Make predictions on test data\n",
    "predicted_classes = model.predict(X_test)\n",
    "print(\"Predicted classes:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model can be a computationally intensive and time consuming process, so we always want to save the model \n",
    "as we train it by creating checkpoints, or as soon as we finish training it by saving the whole model. Since this is \n",
    "a toy example, we will go for the latter using the `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"numpy_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"numpy_model.pkl\", \"rb\") as file:\n",
    "    numpy_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Saving models as pickle files is not adviceable anymore and there are already better ways of doing this using \n",
    "[joblib](https://joblib.readthedocs.io/), [safetensor](https://huggingface.co/docs/safetensors/en/index), [ONNX](https://onnx.ai), \n",
    "and other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(num_samples, input_size)\n",
    "y = torch.randint(0, num_classes, size=(num_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_weights.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the PyTorch code:\n",
    "\n",
    "We generate synthetic data (X and y) similar to the NumPy example.\n",
    "We define the model architecture using the nn.Module class, specifying the layers and activation functions.\n",
    "The forward method defines the forward pass of the model.\n",
    "We create an instance of the model, specifying the input size, hidden size, and number of classes.\n",
    "We define the loss function (cross-entropy loss) and the optimizer (stochastic gradient descent).\n",
    "In the training loop, we perform forward and backward passes, compute the loss, and update the model parameters using the optimizer.\n",
    "PyTorch automatically computes the gradients during the backward pass using automatic differentiation.\n",
    "\n",
    "Using the Tiny Foundation Model with Prompts\n",
    "Once the tiny foundation model is trained, we can use it for various tasks by providing appropriate prompts. For example, let's say we want to classify an image of a handwritten digit. We can create a prompt that guides the model to focus on the relevant features and make a prediction.\n",
    "Prompt: \"Classify the handwritten digit in the image. Look for the overall shape, stroke thickness, and any distinguishing characteristics.\"\n",
    "By providing such prompts, we can guide the model to perform specific tasks, even if it was not explicitly trained for them. This is the power of foundation models – their ability to adapt to new tasks with minimal fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mad_scientist](https://cdn.midjourney.com/b828cd26-b6a7-4086-bb1e-b3c96cf04b8e/0_2.png)\n",
    "\n",
    "Transfer learning is a machine learning technique that leverages knowledge gained from solving one problem and applies it to a different but related problem. The key idea behind transfer learning is to use pre-trained models, which have been trained on large datasets for a specific task, as a starting point for a new task with limited data or resources.\n",
    "In traditional machine learning, models are trained from scratch on a specific dataset for a particular task. This requires a large amount of labeled data and computational resources. Transfer learning, on the other hand, allows us to take advantage of the learned features and patterns from a pre-trained model and adapt them to a new task, even if the new task has a different objective or domain.\n",
    "The process of transfer learning typically involves the following steps:\n",
    "\n",
    "1. Select a pre-trained model: Choose a model that has been trained on a large dataset for a task similar to the target task. Popular pre-trained models include ResNet, VGG, and BERT, which have been trained on datasets like ImageNet or large text corpora.\n",
    "2. Freeze or fine-tune layers: Depending on the similarity between the source and target tasks, you may choose to freeze some or all of the layers in the pre-trained model. Freezing layers means keeping their weights fixed during training, while fine-tuning allows the weights to be updated for the new task.\n",
    "3. Modify the output layer: Replace the output layer of the pre-trained model with a new layer suitable for the target task. For example, if the pre-trained model was used for image classification with 1000 classes and the new task has 10 classes, you would replace the final layer with a new layer having 10 output units.\n",
    "4. Train the model: Train the modified model on the target task dataset. Since the pre-trained model already has learned features, the training process is typically faster and requires less data compared to training from scratch.\n",
    "5. Evaluate and iterate: Assess the performance of the model on the target task and iterate by adjusting hyperparameters, modifying the architecture, or trying different pre-trained models until satisfactory results are achieved.\n",
    "\n",
    "Transfer learning has been successfully applied in various domains, including computer vision, natural language processing, and speech recognition. It has enabled the development of high-performing models even with limited labeled data, making it a valuable technique in scenarios where data acquisition is costly or time-consuming.\n",
    "Metaphor for the Layman:\n",
    "Imagine you are a chef who specializes in Italian cuisine. You have spent years perfecting your pasta-making skills and have a deep understanding of Italian flavors and techniques. Now, you want to expand your repertoire and learn to cook Mexican dishes.\n",
    "Instead of starting from scratch and learning everything about Mexican cuisine from the beginning, you can apply your existing knowledge and skills to adapt to the new cuisine. You already know how to cook pasta, so you can use that knowledge to make similar dishes like tacos or burritos using tortillas instead of pasta. You understand the importance of balancing flavors, so you can apply that principle to create delicious Mexican sauces and seasonings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TinyImageClassifier:\n",
    "    def __init__(self, input_size, hidden_size, num_classes, pretrained_weights=None):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if pretrained_weights is None:\n",
    "            self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "            self.b1 = np.zeros((1, hidden_size))\n",
    "        else:\n",
    "            self.W1 = pretrained_weights['W1']\n",
    "            self.b1 = pretrained_weights['b1']\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, num_classes) * 0.01\n",
    "        self.b2 = np.zeros((1, num_classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.h = np.maximum(0, np.dot(X, self.W1) + self.b1)\n",
    "        scores = np.dot(self.h, self.W2) + self.b2\n",
    "        return scores\n",
    "    \n",
    "    def backward(self, X, y, scores):\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        dscores = probs\n",
    "        dscores[range(num_samples), y] -= 1\n",
    "        dscores /= num_samples\n",
    "        \n",
    "        dW2 = np.dot(self.h.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        \n",
    "        dh = np.dot(dscores, self.W2.T)\n",
    "        dh[self.h <= 0] = 0\n",
    "        \n",
    "        dW1 = np.dot(X.T, dh)\n",
    "        db1 = np.sum(dh, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            scores = self.forward(X)\n",
    "            \n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            correct_logprobs = -np.log(probs[range(num_samples), y])\n",
    "            loss = np.sum(correct_logprobs) / num_samples\n",
    "            \n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, scores)\n",
    "            \n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = self.forward(X)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We modify the __init__ method of the TinyImageClassifier class to accept an optional pretrained_weights parameter. If pretrained_weights is provided, we initialize the weights (W1 and b1) of the first layer with the pre-trained weights instead of random initialization.\n",
    "2. We pre-train the model on the original task using the TinyImageClassifier class, as we did before. This step trains the model on the original dataset (X_train and y_train) and learns the weights (W1 and b1) for the first layer.\n",
    "3. F or transfer learning, we create a new instance of the TinyImageClassifier class called new_model. We specify the input size, a new hidden size (new_hidden_size), and the number of classes for the new task (new_num_classes).\n",
    "4. We pass the pre-trained weights (pretrained_weights) from the first layer of the pre-trained model to the new_model. This initializes the weights of the first layer in the new_model with the learned weights from the pre-trained model.\n",
    "5. We fine-tune the new_model on the new task using a new dataset (X_new and y_new). We typically use a smaller learning rate and fewer epochs for fine-tuning compared to the original training.\n",
    "6. After fine-tuning, we can use the predict method of the new_model to make predictions on new samples (X_test) for the new task.\n",
    "\n",
    "By using the pre-trained weights from the first layer of the original model and fine-tuning them for the new task, we leverage the learned features and adapt them to the specific requirements of the new classification problem. This allows us to benefit from the knowledge learned in the original task and potentially achieve better performance on the new task with less training data and fewer iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train the model on the original task\n",
    "num_samples = 1000\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "X_train = np.random.randn(num_samples, input_size)\n",
    "y_train = np.random.randint(0, num_classes, size=(num_samples,))\n",
    "\n",
    "pretrained_model = TinyImageClassifier(input_size, hidden_size, num_classes)\n",
    "pretrained_model.train(X_train, y_train, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Transfer learning for a new task\n",
    "new_num_classes = 5\n",
    "new_hidden_size = 64\n",
    "\n",
    "pretrained_weights = {\n",
    "    'W1': pretrained_model.W1,\n",
    "    'b1': pretrained_model.b1\n",
    "}\n",
    "\n",
    "new_model = TinyImageClassifier(input_size, new_hidden_size, new_num_classes, pretrained_weights)\n",
    "\n",
    "# Fine-tune the model on the new task\n",
    "X_new = np.random.randn(num_samples, input_size)\n",
    "y_new = np.random.randint(0, new_num_classes, size=(num_samples,))\n",
    "\n",
    "new_model.train(X_new, y_new, num_epochs=5, learning_rate=0.01)\n",
    "\n",
    "# Make predictions on new samples\n",
    "X_test = np.random.randn(10, input_size)\n",
    "predicted_classes = new_model.predict(X_test)\n",
    "print(\"Predicted classes:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "\n",
    "class SummarizationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, output_size):\n",
    "        super(SummarizationModel, self).__init__()\n",
    "        self.embedding = pretrained_model.embedding\n",
    "        self.lstm = pretrained_model.lstm\n",
    "        self.fc = nn.Linear(pretrained_model.lstm.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We define a new class called SummarizationModel that inherits from nn.Module. This model will be used for the summarization task.\n",
    "2. In the __init__ method of SummarizationModel, we take the pre-trained sentiment analysis model (pretrained_model) as a parameter. We initialize the embedding layer and LSTM layer of the summarization model with the corresponding layers from the pre-trained model. This allows us to transfer the learned weights from the sentiment analysis task to the summarization task.\n",
    "3. We replace the final fully connected layer (fc) of the summarization model with a new layer that outputs the desired vocabulary size for summarization (output_size).\n",
    "4. The forward method of SummarizationModel remains similar to the sentiment analysis model, except for the output size.\n",
    "5. We create an instance of the pre-trained sentiment analysis model (sentiment_model) using the same hyperparameters as before.\n",
    "6. We create an instance of the summarization model (summarization_model) by passing the pre-trained sentiment model and the desired output size.\n",
    "7. We fine-tune the summarization model using a new optimizer and loss function specific to the summarization task. We assume that you have the input sequences (X) and corresponding target summaries (y) for training.\n",
    "8. The training loop is similar to the sentiment analysis task, but now we use the summarization_model and the new optimizer and loss function.\n",
    "9. After training, we can use the fine-tuned summarization_model to make predictions on new input sequences (X_test) for summarization. The predicted summaries are obtained by taking the argmax of the model's outputs.\n",
    "\n",
    "By leveraging the pre-trained embeddings and LSTM layers from the sentiment analysis model, we can transfer the learned knowledge to the summarization task. This allows the model to capture important features and patterns from the sentiment analysis task that can be beneficial for summarization.\n",
    "\n",
    "Note that this is a simplified example, and in practice, you may need to make additional modifications based on the specific requirements of your summarization task, such as handling variable-length sequences, using attention mechanisms, or employing more advanced architectures like transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train the sentiment analysis model\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_classes = 2\n",
    "\n",
    "sentiment_model = SentimentModel(vocab_size, embedding_dim, hidden_size, num_classes)\n",
    "\n",
    "# Transfer learning for summarization\n",
    "output_size = 1000  # Vocabulary size for summarization\n",
    "summarization_model = SummarizationModel(sentiment_model, output_size)\n",
    "\n",
    "# Fine-tune the summarization model\n",
    "optimizer = optim.Adam(summarization_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = summarization_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions on new samples\n",
    "X_test = ...  # New input sequences for summarization\n",
    "outputs = summarization_model(X_test)\n",
    "predicted_summaries = outputs.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention_meme](https://i.kym-cdn.com/entries/icons/original/000/036/585/Attention_is_all_you_need.jpg)  \n",
    "Source: [Know Your Meme](https://knowyourmeme.com/memes/attention-is-all-you-need)\n",
    "\n",
    "\n",
    "The Transformers architecture, introduced in the paper [\"Attention Is All You Need\" by Vaswani et al.](https://arxiv.org/abs/1706.03762), \n",
    "has revolutionized the field of natural language processing (NLP) and has since been applied to various other \n",
    "domains, including computer vision and audio. The key innovation of the Transformers architecture is the \n",
    "self-attention mechanism, which allows the model to weigh the importance of different parts of the input \n",
    "sequence when making predictions.\n",
    "\n",
    "The Transformers architecture consists of an encoder and a decoder, each composed of multiple layers. The \n",
    "encoder takes the input sequence and generates a contextualized representation, while the decoder generates the \n",
    "output sequence based on the encoder's output and the previous outputs.\n",
    "\n",
    "The main components of the Transformers architecture are:\n",
    "\n",
    "1. Embedding Layer: The input tokens are converted into dense vector representations using an embedding \n",
    "layer. Positional encodings are added to the embeddings to capture the sequential nature of the input.\n",
    "2. Multi-Head Attention: The self-attention mechanism is applied through multi-head attention. The input \n",
    "sequence is linearly projected into query, key, and value vectors. The attention scores are computed by \n",
    "taking the dot product of the query and key vectors, which determines the importance of each token in the\n",
    "sequence. The attention scores are then used to weight the value vectors, resulting in a weighted sum that\n",
    "captures the relevant information.\n",
    "3. Feed-Forward Neural Network: Each layer in the encoder and decoder also includes a position-wise feed-forward \n",
    "neural network. This network consists of two linear transformations with a ReLU activation in between, applied\n",
    "independently to each position in the sequence.\n",
    "6. Layer Normalization and Residual Connections: Layer normalization is applied after each sub-layer (multi-head \n",
    "attention and feed-forward neural network) to normalize the activations and stabilize training. Residual connections\n",
    "are used to facilitate the flow of information and gradients through the network.\n",
    "8. Decoder: The decoder follows a similar structure to the encoder but includes an additional multi-head attention \n",
    "layer that attends to the encoder's output. This allows the decoder to focus on relevant parts of the input sequence\n",
    "when generating the output.\n",
    "\n",
    "The Transformers architecture has several advantages over previous approaches like recurrent neural networks (RNNs) \n",
    "and convolutional neural networks (CNNs). It can handle long-range dependencies effectively, allows for parallel \n",
    "computation, and scales well to large datasets. The self-attention mechanism enables the model to capture complex \n",
    "relationships between tokens in the sequence, leading to improved performance on various NLP tasks such as machine \n",
    "translation, text summarization, and sentiment analysis.\n",
    "\n",
    "A better, and more fun way to think about them is via the following analogy.\n",
    "\n",
    "Imagine you are a detective trying to solve a complex case. You have a large pile of documents containing \n",
    "information about the case, and your task is to find the relevant pieces of information to crack the case.\n",
    "\n",
    "Instead of reading the documents sequentially from beginning to end, you decide to use a smart approach. You \n",
    "create multiple copies of yourself (multi-head attention) and assign each copy to focus on different aspects \n",
    "of the documents. One copy looks for names, another looks for dates, and another looks for locations. Each \n",
    "copy weighs the importance of each piece of information based on its relevance to the case.\n",
    "\n",
    "After gathering the important information, you and your copies discuss and combine your findings (feed-forward \n",
    "neural network). You then organize and summarize the key points (layer normalization) and add them to your \n",
    "existing knowledge about the case (residual connections).\n",
    "\n",
    "You repeat this process multiple times, each time refining your understanding of the case by focusing on different \n",
    "aspects and combining the information in a meaningful way. Finally, you use all the gathered knowledge to generate \n",
    "a coherent report that solves the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a crucial step in preparing the input data for a Transformer model. It is the process of breaking \n",
    "down a piece of text into smaller units called tokens, which can be individual words, subwords, or even characters. The \n",
    "purpose of tokenization is to convert the raw text into a format that can be easily processed and understood by the model.\n",
    "\n",
    "The tokenization process typically involves the following steps:\n",
    "\n",
    "1. Text Cleaning: The raw text is cleaned by removing unwanted characters, such as punctuation marks, special characters, or HTML tags, depending on the requirements of the task.\n",
    "2. Text Splitting: The cleaned text is split into smaller units called tokens. This can be done using various techniques, such as:\n",
    "   - Word-based tokenization: The text is split into individual words based on whitespace or punctuation.\n",
    "   - Subword tokenization: The text is split into subword units, which can be smaller than words. This helps in handling out-of-vocabulary (OOV) words and reduces the size of the vocabulary. Common subword tokenization algorithms include Byte-Pair Encoding (BPE) and WordPiece.\n",
    "   - Character-based tokenization: The text is split into individual characters, which can be useful for tasks like character-level language modeling.\n",
    "3. Vocabulary Creation: A vocabulary is created based on the tokens obtained from the training data. The vocabulary assigns a unique integer ID to each token, allowing the model to work with numerical representations.\n",
    "4. Token Encoding: Each token in the input text is replaced with its corresponding integer ID from the vocabulary. This step converts the text into a sequence of integers that can be fed into the Transformer model.\n",
    "5. Special Tokens: Additional special tokens are added to the input sequence to provide extra information to the model. Common special tokens include:\n",
    "   - `[CLS]`: A special token added at the beginning of the sequence, typically used for classification tasks.\n",
    "   - `[SEP]`: A special token used to separate different parts of the input, such as sentences or documents.\n",
    "   - `[PAD]`: A special token used for padding the input sequence to a fixed length.\n",
    "   - `[MASK]`: A special token used for masked language modeling tasks, indicating the position of masked tokens.\n",
    "6. Input Formatting: The tokenized and encoded input is formatted into the required input format for the Transformer model, such as a tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "\n",
    "Another way to think of tokenization.\n",
    "\n",
    "Tokenization is like breaking down a large puzzle into smaller, manageable pieces. Imagine you have a 1000-piece puzzle (the raw text), and your goal is to assemble it (process it with the Transformer model). However, working with the entire puzzle at once can be overwhelming and inefficient.\n",
    "\n",
    "So, you start by sorting the puzzle pieces (text cleaning) and grouping them based on their characteristics, such as color or shape (text splitting). You then assign a unique label to each group of pieces (vocabulary creation) to keep track of them easily.\n",
    "\n",
    "Next, you replace each puzzle piece with its corresponding label (token encoding) and add special markers (special tokens) to indicate the beginning, end, or separation of different sections of the puzzle.\n",
    "\n",
    "Finally, you arrange the labeled pieces in a structured manner (input formatting) so that you can easily work with them and start assembling the puzzle (feeding the input to the Transformer model).\n",
    "\n",
    "By breaking down the large puzzle into smaller, labeled pieces, you make the task more manageable and efficient, just like how tokenization helps in processing and understanding text data in a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today we are learning about foundation models!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Transformer model involves the following steps:\n",
    "1. Prepare the input data by tokenizing and converting tokens to dense vector representations using an embedding layer.\n",
    "2. Add positional encodings to capture the sequential nature of the input.\n",
    "3. Pass the input through the encoder, which applies multi-head attention and feed-forward neural networks to generate contextualized representations.\n",
    "4. Pass the encoder's output and previous decoder outputs through the decoder, which generates the output sequence using multi-head attention and feed-forward neural networks.\n",
    "5. Apply layer normalization and residual connections to stabilize training and facilitate information flow.\n",
    "6. Compute the loss function, typically cross-entropy loss, to measure the difference between predicted and target outputs.\n",
    "7. Use an optimizer, such as Adam, to update the model's parameters based on the gradients of the loss function.\n",
    "8. Repeat steps 3-7 for multiple epochs until the model converges or reaches the desired performance.\n",
    "\n",
    "Since training a transformer model is more involved in terms of code complexity and time taken to finish, we will \n",
    "evaluate several training implementations of a transformer.\n",
    "- [hf Q&A example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting is a technique used to guide the Transformer model to perform specific tasks without fine-tuning. It involves providing a task-specific prompt or instruction along with the input sequence. The model then generates the output based on the given prompt. Prompting is effective for tasks such as text generation, question answering, and sentiment analysis. It allows for quick adaptation to new tasks without the need for extensive training data or model modifications.\n",
    "\n",
    "We can create prompts with straightforward text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use specialized tools like guidance, outlines, sglang, dspy or instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enhance prompts using Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to keep in mind is the prompt structure your model is expecting (if it is a text-to-something model).\n",
    "\n",
    "Some examples include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning is the process of adapting a pre-trained Transformer model to a specific downstream task. It involves the following steps:\n",
    "1. Initialize the model with pre-trained weights from a large-scale corpus.\n",
    "2. Replace the original output layer with a new layer specific to the downstream task.\n",
    "3. Freeze the weights of the pre-trained layers to preserve the learned representations.\n",
    "4. Train the model on the downstream task dataset, updating only the weights of the new output layer and optionally the top few layers of the model.\n",
    "5. Evaluate the fine-tuned model on the task-specific validation set and adjust hyperparameters if necessary.\n",
    "Fine-tuning allows the model to leverage the knowledge learned from pre-training and adapt it to specific tasks with limited training data.\n",
    "\n",
    "Example using https://www.kaggle.com/datasets/jorgeruizdev/ludwig-music-dataset-moods-and-subgenres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"audiofolder\", \n",
    "    data_dir=\"data/\",\n",
    "    drop_metadata=True,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine tune a music classifier, we will use [wav2vec](https://arxiv.org/abs/2006.11477) from Meta as a our base model.\n",
    "\n",
    "> Wav2vec is a self-supervised speech recognition model developed by Meta AI (formerly Facebook AI) that can learn powerful speech representations from raw audio data with little to no transcribed speech.\n",
    "\n",
    "The key aspects of wav2vec are:\n",
    "- Wav2vec uses a self-supervised approach to learn speech representations directly from raw audio without relying on large amounts of transcribed speech data.\n",
    "- Similar to BERT for text, wav2vec is trained by masking parts of the raw audio and learning to predict the masked speech units.\n",
    "- Wav2vec automatically discovers speech units (shorter than phonemes) that are used to represent the speech audio sequence.\n",
    "- By leveraging large amounts of unlabeled speech data through self-supervision, wav2vec can outperform traditional supervised speech recognition models that rely solely on transcribed audio, even when using 100 times less labeled data.\n",
    "\n",
    "The key motivation behind wav2vec is to make speech technology accessible to more languages and dialects by reducing the dependence on transcribed data, which is scarce for most languages. Meta AI has open-sourced the pretrained models and code to enable wider adoption and research in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    return feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True, padding=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_latin = dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_latin[\"train\"].features[\"input_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(encoded_latin[\"train\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(id2label)\n",
    "num_labels, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_latin[\"train\"],\n",
    "    eval_dataset=encoded_latin[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"audio-classification\", model=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "audio_file = dataset[\"train\"][choice(range(5))][\"audio\"][\"path\"]\n",
    "audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier.predict(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as Audio2\n",
    "\n",
    "Audio2(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, not well, but it works! 😎👌🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other thing to keep in mind or to do while fine-tuning, but going through these is out of the scope of this session so I highly encourage you to check out the resources below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Reinforcement Learning with Human Feedback (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rlhf](https://assets-global.website-files.com/63024b20439fa6bd66ee3465/657a79cf031c15004cc74699_Thumbnail.PNG)\n",
    "\n",
    "RLHF is an approach to align language models with human preferences. It involves the following steps:\n",
    "1. Collect a dataset of human-generated responses to a given set of prompts.\n",
    "2. Train a reward model to predict the quality of the model's outputs based on human feedback.\n",
    "3. Use the reward model to provide feedback to the language model during the training process.\n",
    "4. Update the language model's parameters based on the rewards received, encouraging it to generate outputs that align with human preferences.\n",
    "RLHF helps to mitigate issues like bias, toxicity, and hallucinations in generated outputs, making the models more reliable and safe for real-world applications.\n",
    "\n",
    "RLFH can be a much more involved a step as the triaining of the model itself, therefore, we won't be cover this step here but \n",
    "you will see resources below for you to learn more about RLHF.\n",
    "- [LLM Training: RLHF and Its Alternatives by SEBASTIAN RASCHKA, PHD](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)\n",
    "- [Stanford CS224N | 2023 | Lecture 10 - Prompting, Reinforcement Learning from Human Feedback](https://www.youtube.com/watch?v=SXpJ9EmG3s4)\n",
    "- [Reinforcement Learning from Human Feedback: From Zero to chatGPT](https://www.youtube.com/watch?v=2MBJOuVq380)\n",
    "- [Reinforcement Learning from Human Feedback course by DeepLearning.ai](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the success of Transformers, there are several challenges:\n",
    "\n",
    "1. Limited Context: Transformers have a fixed context window, limiting their ability to process long-range dependencies beyond the window size.\n",
    "\n",
    "![context window](https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2023/11/robot-llm-long-context.jpg?ssl=1)  \n",
    "Source: [StreamingLLM gives language models unlimited context by Ben Dickson](https://bdtechtalks.com/2023/11/27/streamingllm/)\n",
    "\n",
    "2. Lack of Interpretability: The complex nature of self-attention makes it difficult to interpret the model's decisions and reasoning.\n",
    "\n",
    "![explainthyself](https://miro.medium.com/v2/resize:fit:2000/1*nJinotXOj6DY4xtss2cxpA.png)\n",
    "\n",
    "3. Bias and Fairness: Transformer models can inherit biases present in the training data, leading to biased or unfair outputs.\n",
    "\n",
    "![bias](https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2024/02/Screenshot-2024-02-22-at-8.18.28-AM-800x450.png)\n",
    "\n",
    "4. Out-of-Distribution Generalization (aka Hallucinations): Transformers may struggle to generalize well to data that is significantly different from the training distribution.\n",
    "\n",
    "5. Out-of-Date Knowledge\n",
    "\n",
    "![out_of_date](https://global.discourse-cdn.com/openai1/original/3X/5/6/56b607844d56bd6017ce7a9d8b39fb557becdb9b.png)\n",
    "\n",
    "6. Batching\n",
    "\n",
    "![challenge](https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png)\n",
    "\n",
    "7. Infrastructure: Training Transformer models requires significant computational resources due to their large size and the need for extensive pre-training.\n",
    "![costs](https://www.nextplatform.com/wp-content/uploads/2022/12/cerebras-model-training-price-table-TNP.jpg)  \n",
    "Source: [Counting the Cost of Training Large Language Models by Timothy Prickett Morgan](https://www.nextplatform.com/2022/12/01/counting-the-cost-of-training-large-language-models/)\n",
    "    1. Costs\n",
    "    2. Scarcity\n",
    "    3. Expertise\n",
    "    4. Lock-In\n",
    "\n",
    "Researchers are actively working on addressing these challenges to improve the efficiency, \n",
    "interpretability, and robustness of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies for Deployment and Post-Deployment Maintenance\n",
    "\n",
    "\n",
    "\n",
    "Here's the process expressed as a mermaid diagram:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Collect Data] --> B[Engineer Features]\n",
    "    B --> C[Train and Evaluate the Model]\n",
    "    C --> D[Evaluate Model]\n",
    "    D --> B\n",
    "    D --> E[Serialize and Package the Model]\n",
    "    D --> F[Choose a Deployment Architecture]\n",
    "    E --> G[Containerize the Model]\n",
    "    F --> G\n",
    "    G --> H[Deploy the Model]\n",
    "    H --> I[Expose the Model Endpoint]\n",
    "    I --> J[Monitor and Maintain]\n",
    "    J --> B\n",
    "```\n",
    "\n",
    "This diagram illustrates the high-level steps involved in the machine learning \n",
    "lifecycle, from training and evaluation to deployment, exposure, and maintenance. Each \n",
    "step plays a crucial role in ensuring the model is effectively served and can be \n",
    "accessed by the intended consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model([\"this is a long story from a king 1000 years ago\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p servers/summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlserver](https://mlserver.readthedocs.io/en/latest/_images/mlserver_setup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/summarizer/t5_model.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "class Summarizer(MLModel):\n",
    "    async def load(self):\n",
    "        self.model = pipeline(\"summarization\", model=\"t5-small\", device=)\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, text: List[str]) -> List[str]:\n",
    "        return [model(text)['summary_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/summarizer/model-settings.json\n",
    "{\n",
    "    \"name\": \"summarizer\",\n",
    "    \"implementation\": \"t5_model.Summarizer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the terminal, start your service with the following command.\n",
    "\n",
    "```sh\n",
    "mlserver start servers/summarizer\n",
    "```\n",
    "\n",
    "Now we can test our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('MyMovieEval (example@example.com)', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barbie = wiki_wiki.page('Barbie_(film)').summary\n",
    "oppenheimer = wiki_wiki.page('Oppenheimer_(film)').summary\n",
    "\n",
    "print(barbie)\n",
    "print()\n",
    "print(oppenheimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text_inputs\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [article],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:8080/v2/models/summarizer/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(endpoint, json=inference_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/translator/en_to_es.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "class Translator(MLModel):\n",
    "    async def load(self):\n",
    "        self.model = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, payload: List[str]) -> List[str]:\n",
    "        model_output = self.model(payload, min_length=5, max_length=100)\n",
    "        return [model_output[0]['translation_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/translator/model-settings.json\n",
    "{\n",
    "    \"name\": \"translator\",\n",
    "    \"implementation\": \"en_to_es.Translator\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/settings.json\n",
    "{\n",
    "    \"http_port\": 7070,\n",
    "    \"grpc_port\": 7090,\n",
    "    \"metrics_port\": 7080\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the terminal, start your service with the following command.\n",
    "\n",
    "```sh\n",
    "mlserver start servers\n",
    "```\n",
    "\n",
    "Now we can test our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:8080/v2/models/summarizer/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(endpoint, json=inference_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What we are up to at Seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle some of the challenges we touched on (i.e., )\n",
    "\n",
    "![slide3](images/slide_3.png)\n",
    "![slide4](images/slide_4.png)\n",
    "![slide5](images/slide_5.png)\n",
    "![slide6](images/slide_6.png)\n",
    "![slide7](images/slide_7.png)\n",
    "![slide8](images/slide_8.png)\n",
    "![slide9](images/slide_9.png)\n",
    "![slide10](images/slide_10.png)\n",
    "![slide11](images/slide_11.png)\n",
    "![slide12](images/slide_12.png)\n",
    "![slide13](images/slide_13.png)\n",
    "![slide14](images/slide_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If anymore questions come up, of if you'd just like to chat about MLOps, please join our community \n",
    "[at the link here](https://seldondev.slack.com/join/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg#/shared-invite/email)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
